{
  "model_name_or_path": "llama-2-7b",
  "output_dir": "../models/corp-llm-loRA",
  "batch_size": 8,
  "micro_batch_size": 4,
  "num_epochs": 3,
  "learning_rate": 3e-4,
  "lora_rank": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "max_seq_length": 2048
}